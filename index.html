<!DOCTYPE html>
<html>
<head>
  <style>
    body {
      background-color:lightblue;
      }
    h1 {
      text-align:center;
      font-size:35px;
      }
    h2 {
      text-align:center;
      font-size:30px;
      }
    h3 {
      margin-left:250px;
      text-align:left;
      font-size:25px;
      }
    p {
      margin-right:300px;
      margin-left:300px;
      text-align:left;
      font-size:20px;
      }
    img {
    height:300px;
    width:400px;
    }
    ul {
      margin-right:300px;
      margin-left:300px;
      text-align:left;
      font-size:18px;
      padding: 0;
    }
    h3.dataset-header {
      margin-right:250px;
      margin-left:250px;
      text-align:left;
      font-size:20px;
      padding: 0;
    }

    .center {
    display:block;
    margin-left:auto;
    margin-right:auto;
    width:50%;
    }
  </style>
</head>
<body>
<center>
  <section class="page-header">
      <h1 class="project-name">CS4641</h1>
      <h2 class="project-tagline">Predicting March Madness</h2>
      <h2 class="project-tagline2">Nikhil Bose, Joel Joseprabu, Ji Won Lee, and Sandeep Veludhandi</h2>
      <h3 class="project-tagline2"><a href="files/Project Proposal.pdf">Project Proposal</a></h3>
  </section>
  <section class="main-content">
  <h3>Introduction and Motivation</h3>
    <p><img src = "images/marchMadness.jpg" alt = "March Madness" class = center></p>
  <p>The March Madness Tournament is the biggest college basketball tournament in America.
     Millions of people fill out brackets in hopes of correctly predicting the tournament and winning competitions and bets.
     However, in all the years the tournament has occurred, not a single person has created the perfect bracket yet.</p>
  <p>Being able to predict a tournament correctly would be revolutionary in the world of sports betting.
    It would also allow insight in basketball analysis. If some stats are more important than others and lead to more wins on average,
    coaches would know what the best way is to practice. Basketball is an extremely complicated sport where all sorts of upsets could
    happen. By using machine learning techniques, such as neural networks and random forests, we hope to find what causes a team to push
    through a tournament and win it all.</p>
  <h3>Dataset</h3>
  <p>The dataset was acquired from Kaggle's 2019 March Madness competition. The dataset had recorded games from 2003-current and included
    regular season as well as tournament games, totaling to over 88000 games. The dataset included 14 features such as free throws made,
    steals, and offensive rebounds. For each game, the stats for each of those features was recorded and used in our analysis. Fortunately,
    most the data was already quantified so we didn’t have to do it ourselves. The only data we had to quantify was the location of the game.
    We did this by having -1 represent an away game, 0 a neutral game, and 1 a home game.</p>
  <h3 class="dataset-header">Our models used the following features for each team:</h3>
  <ul style="list-style-type:disc;">
    <li>Points Allowed</li>
    <li>Points Scored</li>
    <li>Field Goals Made</li>
    <li>Field Goals Attempted</li>
    <li>3 Pointers Made</li>
    <li>Offensive Rebounds</li>
    <li>Defensive Rebounds</li>
    <li>Assists</li>
    <li>Location of the Game (Home, Away, or Neutral)</li>
    <li>Personal Fouls</li>
    <li>Steals</li>
    <li>Blocks</li>
    <li>Turnovers</li>
    <li>Free Throws Made</li>
  </ul>
  <h3 class="dataset-header">The different sets of features we ran our models with are listed below:</h3>
  <table>
  <tr>
    <th>Feature Set Number</th>
    <th>Features</th>
  </tr>
  <tr>
    <td>1</td>
    <td>
      <ul style="list-style-type:disc;">
        <li>Points allowed</li>
        <li>Points scored</li>
        <li>Field goals attempted</li>
        <li>3 pointers made</li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>2</td>
    <td>
      <ul style="list-style-type:disc;">
        <li>Offensive rebounds</li>
        <li>Defensive rebounds</li>
        <li>Assists</li>
        <li>3 pointers made</li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>3</td>
    <td>
      <ul style="list-style-type:disc;">
        <li>Steals</li>
        <li>Blocks</li>
        <li>Defensive rebounds</li>
        <li>Personal fouls</li>
        <li>Location of the game</li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>4</td>
    <td>
      <ul style="list-style-type:disc;">
        <li>Assists</li>
        <li>Turnovers</li>
        <li>Points scored</li>
        <li>Points allowed</li>
        <li>3 pointers made</li>
      </ul>
    </td>
  </tr>
    <tr>
    <td>5</td>
    <td>
      <ul style="list-style-type:disc;">
        <li>Free throws made</li>
        <li>2 pointers made</li>
        <li>Field goals made</li>
        <li>Assists</li>
        <li>Points scored</li>
        <li>Personal fouls</li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>6</td>
    <td>
      <ul style="list-style-type:disc;">
        <li>Steals</li>
        <li>Blocks</li>
        <li>Defensive rebounds</li>
        <li>Personal fouls</li>
        <li>Assists</li>
        <li>Field goals made</li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>7</td>
    <td>
      <ul style="list-style-type:disc;">
        <li>Location of the game</li>
        <li>Points scored</li>
        <li>field goals made</li>
        <li>field goals attempted</li>
        <li>3 pointers made</li>
        <li>Free throws made</li>
        <li>Offensive rebounds</li>
        <li>Assists</li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>8</td>
    <td>
      <ul style="list-style-type:disc;">
        <li>Location of the game</li>
        <li>Points scored</li>
        <li>Points allowed</li>
        <li>field goals made</li>
        <li>field goals attempted</li>
        <li>3 pointers made</li>
        <li>Free throws made</li>
        <li>Offensive rebounds</li>
        <li>Assists</li>
        <li>Steals</li>
        <li>Blocks</li>
        <li>Defensive rebounds</li>
        <li>Personal fouls</li>
      </ul>
    </td>
  </tr>
  </table>
  <h3>Approach</h3>
  <p>We evaluated all the games using different subsets of features. One subset could be using 3 pointers made, turnovers, and field goals
    made, and score, while another subset could be fouls, assists, and defensive rebounds. We used sets of features that we thought would
    perform the best and compared the results against other sets.</p>
  <p>The different supervised learning models we used were Logistic Regression, Random Forests, Naïve Bayes, and Neural Networks.
    Additionally, we used XGBoost as a means of increasing the accuracy of our results as well as incorporating a new feature, seeding,
    when we construct our bracket. Since we had multiple models that worked well, we decided to implement a new method which would pick the
    most confident choice out of all the models.</p>
  <h5>Logistic Regression</h5>
  <p>Logistic Regression is a discriminative model that calculates the probability of a data point being in a specific class, in this case
    the probability of a team winning or losing.</p>
    <p><img src = "images/logisticRegressionAccuracy.png" alt = "Logistic Regression Accuracy" class = "center"></p>
  <h5>Random Forests</h5>
  <p>Random Forests is an ensemble method performed by bagging multiple random decision trees.  Bagging combines multiple decision trees
    to reduce the final variance which should improve the accuracy of the model.</p>
    <p><img src = "images/randomForestAccuracy.png" alt = "Random Forest Accuracy" class = "center"></p>
  <h5>Naïve Bayes</h5>
  <p>Naïve Bayes is a generative model working under the assumption that features are independent and classifies the data regardless of
    correlations between features.</p>
    <p><img src = "images/naiveBayesAccuracy.png" alt = "Naive Bayes Accuracy" class = "center"></p>
  <h5>Neural Networks</h5>
  <p>Neural Networks use a multitude of different nodes that sum up a series of weights for all the features to produce an output.</p>
    <p><img src = "images/neuralNetworkAccuracy.png" alt = "Neural Network Accuracy" class = "center"></p>
  <h5>XGBoost</h5>
  <p>XGBoost is an ensemble learning method similar to random forests in that it uses decision trees but uses gradient boosting. It
    helps to minimize both bias and variance by using a multitude of decision trees.
  </p>
  <h4>Seeding</h4>
  <p>Upsets in basketball occur when a higher seeded team loses to a much worse team. Of the 63 games played over the course of each
    tournament since 1985, there has been on average 12 upsets per tournament. Due to this, we created a higher threshold for lower
    seeded teams to win; teams that are lower seeded need to have higher than a 50% chance to win in order to move on in the bracket.</p>
  <h3>Results</h3>
  <h4>Seeding</h4>
  <p>Our implementation of seeding has been proven to be quite useful. Given a equal probability distribution (which scored 12 points for its prediction), the seeding implementation predicted a bracket that scored 76. That's roughly a 500% improvement!</p>
  <h4>XG Boosting</h4>
  <p>XGBoosting was </p>
  <h4>Combining our models: Pick Most Confident</h4>
  <p>We wrote a script that would compare bracket predictions from our best models, which were logistic regression and neural networks. The script would compare the predictions in the two models and choose the prediction that had higher certainty, meaning the difference between winning rate of the two teams. The reason behind such decision was that the two models, logistic regression and neural networks had similar accuracy so we tested if combining the two models would improve our accuracy. The log loss of the model did not change much but the brackets have improved accuracy by an average of _INSERT INFO___ . _INSERT STATISTIC_</p>
  <h4>Overall result</h4>
  <p>We evaluated each model using the log-loss for the 2018-2019 March Madness tournament as well as how many points the bracket would
    generate in the NCAA competition. Logistic Regression, along with Neural Networks, performed the best out of the first four
    techniques and usually had a very close accuracy with Neural Networks. This is most likely the case because of the Neural Network’s
    own logistic regression it uses to calculate the weights. Out of all the feature sets, Naïve Bayes consistently performed the worst
    and had the worst accuracy on the training data. Most notably, the classifier was extremely confident in all of its predicted matchup
    s, even though it predicted many of them incorrectly. Random Forests consistently performed worse than both Neural Networks and
    Logistic Regression but better than Naïve Bayes.</p>
    <p><img src = "images/allModelAccuracies.png" alt = "All Accuracies" class = "center"></p>
  </section>
</center>
</body>
</html>
